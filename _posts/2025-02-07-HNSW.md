---
layout: post
title: "Hierarchical Navigable Small Worlds (HNSW)"
date: 2025-02-07
excerpt: "HNSW is an approximate nearest neighbor search that efficiently searches for nearest neighbors using greedy traversal and scales well with large datasets."
---

## **Hierarchical Navigable Small Worlds (HNSW)**
HNSW is a clever data structure and algorithm for approximate nearest neighbor (ANN) search that leverages a hierarchical, navigable small-world graph structure. It efficiently searches for nearest neighbors using greedy traversal and scales well with large datasets.

### **Hierarchical**
- Stacked subgraphs of exponentially decreasing density.  
- Higher layers contain fewer nodes with long-range links, while lower layers contain more nodes with short-range links.

### **Navigable**
- Search complexity scales polylogarithmically using decentralized greedy search.  
- Average search complexity:  
  $$
  O(\log n \log k)
  $$
  where $$ n $$  is the number of nodes, and $$ k $$ is the average degree of nodes.

### **Small-World Property**  
- HNSW forms a unique graph with:  
  - Low average shortest path length (L) → Ensures quick navigation.  
  - High clustering coefficient (C) → Ensures local connectivity for better accuracy.  

### **Why HNSW Became Popular**  
- Effective for vector similarity search (e.g., in recommendation systems, image retrieval, and NLP embeddings).  
- Based on proximity graphs, where vertices are linked based on a distance metric (e.g., Euclidean distance, cosine similarity).  
- Uses two distinct methods on top of the proximity graph:  
  - Probabilistic Skip List
  - Navigable Small World  

## **Two Key Techniques in HNSW**  

### **1. Probabilistic Skip List**
- Extension of a linked list into multiple levels.  
- Builds subsequent layers probabilistically, where higher layers contain fewer elements.  
- Higher layers skip more neighbors, while lower layers store shorter-range edges.  
- Helps balance long-range and short-range connectivity.  

### **2. Navigable Small World Graphs**
- Constructs a proximity graph with both long-range and short-range connections.  
- Search follows a greedy routing process:  
  1. Start at a predefined entry point.  
  2. Move to the vertex closest to the query vector.  
  3. Repeat the greedy search to navigate the graph.  
  4. Stop when reaching a local minimum (no closer vertex).  

- Challenges:
  - Greedy routing fails for large networks if the graph is not sufficiently navigable.  
  - To improve performance, one can increase the average degree of nodes, which increases accuracy but at the cost of higher memory usage and search complexity.  

### **HNSW: Combining 1 & 2**
HNSW is a hierarchy of NSW graphs:
- Top Layer: Contains longest links (low-density, high-degree vertices).  
- Bottom Layer: Contains shortest links (dense, low-degree vertices).  
- Search process:
  1. Zoom-out phase : Start from the top layer, using long-range edges for fast traversal.  
  2. Zoom-in phase : Move down layers, refining the search with shorter-range edges.  
  3. Stop when reaching a local minimum at the lowest level.  

This process is analogous to a machine learning optimization process:
- Higher layers behave like a high learning rate, making broad jumps.  
- Lower layers behave like a low learning rate, refining the search to the nearest neighbor.  

### **Summary of Pros & Cons**
### Pros
1. Fast Search Speed:  
   - Sub-linear time complexity: \( O(\log n \times \log k) \).  
   - Efficient for high-dimensional data.  
2. High Recall & Accuracy:  
   - Outperforms many tree-based and hashing-based methods in precision.  
3. Scalability:  
   - Works well with millions of high-dimensional vectors.  
4. Dynamic Updates:  
   - Supports real-time insertions and deletions, unlike many ANN methods.  
5. Memory Efficient Graph Structure:  
   - Does not store exhaustive pairwise distances but only necessary edges.  

### Cons
1. High Memory Usage:  
   - More memory-intensive than tree-based approaches due to storing multiple graph layers.  
2. Slow Index Construction:  
   - Building the graph is computationally expensive compared to simpler methods.  
3. Parameter Sensitivity:  
   - Performance depends on the tuning of parameters like `M` (max connections per node) and `ef` (size of candidate search list).  
4. Not Ideal for Low-Dimensional Data:  
   - Tree-based structures (e.g., KD-trees) may outperform HNSW in very low-dimensional spaces.  


### References
[Amazing Post by brtholomy](https://github.com/brtholomy/hnsw)

[Malkov and Yashunin, 2016 (HNSW Paper)](https://arxiv.org/abs/1603.09320)