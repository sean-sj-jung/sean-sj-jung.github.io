---
layout: post
title: "Rotary Position Embedding (RoPE)"
date: 2024-12-30
excerpt: "Overview of Rotary Position Embedding (RoPE)"
---

### Positional Encoding
- Traditional absolute positional encoding often resorts to adding or concatenating fixed vectors to embeddings.
- Limits sequence length generalization.
- Does not explicitly encode relative position differences in attention computations.
  - E.g., attention treats d(pos0, pos1) the same as d(pos1, pos4095).


### RoPE
- Instead of adding position embeddings, RoPE applies multiplicative rotation to the query and key embeddings before attention computation.
- Uses complex number rotations (or equivalently sine/cosine) to rotate each pair of dimensions.
    - If 512 dimensions, then rotation is applied to 256 two-dimensional planes.
- Relative Position Sensitivity: Naturally preserves positional differences in attention computation.
- Long-term decay: *"the inner-product will decay when the relative position increase"*
- Generalizes well to longer sequences than seen during training.


### References
[Su et al., 2023](https://arxiv.org/pdf/2104.09864)

[RoPE Implementation in Meta's LLaMa](https://github.com/meta-llama/llama/blob/main/llama/model.py#L80)