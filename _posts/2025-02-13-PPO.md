---
layout: post
title: "RL and PPO"
date: 2025-02-13
excerpt: "RLHF and Proximal Policy Optimization"
---


### **RLHF**  
1.	Generate a dataset of prompts (e.g., questions) and multiple candidate responses for each prompt.  
2.	Have humans rank the responses by quality or preference.  
3.	Train a reward model to predict human rankings — it learns to score responses in line with human preferences.  
4.	Use reinforcement learning (with PPO/GRPO) to fine-tune the policy model (the main LLM) to maximize the reward model’s score for its generated responses.   


### Reward Model

Pairwise loss : Encourages the model to score preferred answers higher than less preferred ones.  

#### Bradley-Terry or Logistic Loss  
Given a pair of responses $$r^+, r^-$$ to a prompt (where $$r^+$$ is the preferred answer and $$r^-$$ is the less preferred one, the reward model assigns scores:  

$$s^+ = R_\theta(r^+), \quad s^- = R_\theta(r^-)$$

The loss function is:

$$\mathcal{L}(\theta) = -\log \sigma(s^+ - s^-)$$

where $$\sigma$$ is the sigmoid function:

$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

Intuition :
- The model is penalized if it assigns a higher score to the less preferred response.
- The sigmoid ensures the loss smoothly approaches zero when s^+ \gg s^-.

### **Proximal Policy Optimization (PPO)**

A Popular algorithm used in RLHF for LLM.

