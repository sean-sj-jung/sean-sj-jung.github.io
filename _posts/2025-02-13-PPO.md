---
layout: post
title: "RL and PPO"
date: 2025-02-13
excerpt: "RLHF and Proximal Policy Optimization"
---

### RL terminologies
State : $$s_t$$  
Action : $$a_t$$  
State and Action are at token level. Aggregation of all tokens for each instance $$ i $$ becomes a response, $$r_i$$  
  

### **RLHF**  
1.	Generate a dataset of prompts (e.g., questions) and multiple candidate responses for each prompt.  
2.	Have humans rank the responses by quality or preference.  
3.	Train a reward model to predict human rankings — it learns to score responses in line with human preferences.  
4.	Use reinforcement learning (with PPO/GRPO) to fine-tune the policy model (the main LLM) to maximize the reward model’s score for its generated responses.   
  

### Reward Model
Supervised Fine-tuned model but with pairwise loss instead of cross-entropy loss.  
Pairwise loss : Encourages the model to score preferred answers higher than less preferred ones.  
  

#### Bradley-Terry or Logistic Loss  
Given a pair of responses $$r^+, r^-$$ to a prompt (where $$r^+$$ is the preferred answer and $$r^-$$ is the less preferred one, the reward model assigns scores:  

$$s^+ = R_\theta(r^+), \quad s^- = R_\theta(r^-)$$

The loss function is:

$$\mathcal{L}(\theta) = -\log \sigma(s^+ - s^-)$$

where $$\sigma$$ is the sigmoid function:

$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

Intuition :
- The model is penalized if it assigns a higher score to the less preferred response.
- The sigmoid ensures the loss smoothly approaches zero when $$s^+ \gg s^-$$.

### **Proximal Policy Optimization (PPO)**

A Popular algorithm used in RLHF for LLM  

Three components of PPO:   
- Policy : A primary model to be trained
- Reward model : A frozen model that provides scalar reward for response to a prompt
- Critic : value function, a learnable function that takes partial response to a prompt and predicts the scalar reward

Training a policy model:  
1. Generate responses from policy model
2. Genereate rewards for each response
3. GAE to compute advantage
  - GAE : General Advantage Estimation
    - How much better a specific action $a_t$ is compared to an average action the policy will take in state $$s_t$$
    - $$ A_t = Q(s_t, a_t) - V(s_t) $$
    Where
    $$ Q(s_t, a_t) $$ : expected reward of $$a_t$$ at $$s_t$$  
    $$ V(s_t) $$ : average reward of $$s_t$$ over $$a_t$$  
    - The advantage is often estimated using either: 
      - Monte-Carlo  
        - High variance due to sparse reward
        - Expensive to sample from LLM
        - But low bias and allow accurate estimate of reward
      - Temporal Difference learning  
        - Compute reward at a token level (low variance)
        - High bias and low accuracy in final reward from partially generated response
      - GAE
        - Balance of bias and variance through multi-step Temporal Difference  
        - A reward is 0 if the response is incomplete
          - "Critic" is trained to anticipate the final reward given only a partial state
4. Update policy
5. Update critic

### Critic, Value function  
A critic is trained to anticipate the final reward given only a partial state which allows to compute the temporal difference. 



### Reference
[HuggingFace's blog](https://huggingface.co/blog/deep-rl-ppo)  
[Post by Jimmy Shi](https://yugeten.github.io/posts/2025/01/ppogrpo/)  
