---
layout: post
title: "Group Relative Policy Optimization"
date: 2025-02-14
excerpt: "Overview of Group Relative Policy Optimization (GRPO)"
---

# **Group Relative Policy Optimization (GRPO)**

### Summary
- No separate value network (Critic)
  - Traditional Proximal Policy Optimization (PPO) requires a Critic model, which is typically as large as the Actor model, doubling the resource requirements.
  - GRPO eliminates the need for a Critic, significantly reducing memory and compute costs.

- Sampling multiple outputs from the old policy
  - For each query $$ q $$, a set of outputs $$ o_i $$ is sampled from the previous policy.

- Reward assignment and normalization
  - A reward model assigns reward values $$ r_i $$ to each sampled output $$ o_i $$.
  - Rewards are normalized within the group using:  
$$
\tilde{r} = \frac{r_i - \text{mean}(r)}{\text{std}(r)}
$$
  - This normalization helps ensure that advantages are computed relative to other outputs, improving stability.

- Established optimization techniques
  - Clipping prevents excessively large updates to the policy.
  - KL divergence regularization controls divergence from the reference policy, preventing instability.

### Limitations
- GRPO has primarily been evaluated on mathematical reasoning and coding tasks.
- Its effectiveness on general language modeling or broader reinforcement learning problems is still unclear.
