---
layout: post
title: "Random topics on Neural Network"
date: 2024-11-01
excerpt: "Stochastic Gradient Descent, Embedding, and Softmax"
---

### Stochastic Gradient Descent (SGD) vs. Gradient Descent (GD)

**Advantages**  

- Computationally efficient for large datasets  
- Better exploration of the loss surface, helping avoid saddle points and poor local minima  
- Allows online learning when data arrives sequentially  

**Disadvantages**  

- Noisy updates introduce variance, making convergence less stable  
- Requires careful hyperparameter tuning (e.g., learning rate scheduling)  
- Higher variance in updates leads to results varying across runs  

  

### Embedding Layer (and tie_word_embedding)
An embedding layer is a computationally efficient lookup table that maps token indices to dense vector representations. 

Given an input $$x$$, it is tokenized into token indices $$z$$ using a tokenizer. The embedding function then outputs a matrix of shape:

$$ \text{embedding}(z) \rightarrow (\text{max_len}, d_{\text{model}}) $$

where:
- $$\text{max_len}$$ is the maximum sequence length (also called context length).
- $$d_{\text{model}}$$ is the dimension of the model/embedding.

Thus, the embedding layer effectively acts as a lookup table of shape:

$$(\text{vocab_size}, d_{\text{model}})$$

where $$\text{vocab_size}$$ is the tokenizer's vocabulary size.  

**Tied Word Embeddings** refers to weight sharing between the embedding layer and the final projection layer (often called `lm_head`). Instead of having separate parameters for both, the embedding matrix $$ (\text{vocab_size}, d_{\text{model}}) $$ is transposed and reused in the output layer $$ (d_{\text{model}}, \text{vocab_size}) $$. This reduces the number of parameters and improves generalization.  
  

### Softmax and NLP
The softmax function converts raw output logits into a probability distribution over possible classes. It ensures that probabilities sum to 1, making it useful for classification.

$$
\sigma(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{N} e^{z_j}}
$$

where:
- $$z_i$$ is the logit (raw score) from the model's output for class $$i$$.
- $$e^{z_i}$$ exponentiates the logit, ensuring positive values.
- The denominator normalizes the probabilities across $$N$$  classes (e.g., vocabulary words in NLP).
- The output is a valid probability distribution where $$ \sum \sigma(z_i) = 1 $$.

### Cross-Entropy Loss
Cross-entropy loss measures the difference between the true and predicted probability distributions. It is defined as:

$$
L = -\sum_{i=1}^{N} y_i \log(\hat{y}_i)
$$

where:
- $$N$$ is the number of classes.
- $$y_i$$ is the true label (one-hot encoded, 1 for the correct class, 0 otherwise).
- $$ \hat{y}_i $$ is the predicted probability for class $$ i $$ (from softmax).
- The negative log penalizes incorrect predictions exponentially, encouraging higher confidence in correct predictions.

For a batch of size  $$M$$, the average cross-entropy loss is:

$$
L = -\frac{1}{M} \sum_{j=1}^{M} \sum_{i=1}^{N} y_{ji} \log(\hat{y}_{ji})
$$

where $$j$$ indexes batch samples.


### KL Divergence  

Kullbackâ€“Leibler (KL) divergence measures how one probability distribution diverges from another, defined as:

**Discrete case:**
$$
D_{KL}(P \parallel Q) = \sum_{x \in X} P(x)\log\frac{P(x)}{Q(x)}
$$

**Continuous case:**
$$
D_{KL}(P \parallel Q) = \int_{-\infty}^{\infty} P(x)\log\frac{P(x)}{Q(x)}\,dx
$$

### Properties
- **Asymmetric**: $$D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P)$$
- **Non-negative**: $$D_{KL}(P \parallel Q) \geq 0$$, equality if and only if $$P = Q$$

## Applications in Neural Networks

### 1. Variational Autoencoders (VAEs)
- Encourages approximate posterior $$Q(z|x)$$ to remain close to prior $$P(z)$$
- Loss:
$$
\text{Loss} = \text{Reconstruction Loss} + D_{KL}(Q(z|x)\parallel P(z))
$$

### 2. Knowledge Distillation
- Helps student network imitate teacher network's soft-label distributions
- Ensures close alignment between student and teacher predictions

### 3. Reinforcement Learning (Policy Gradients)
- Controls update magnitude of policies (e.g., TRPO, PPO)
- Stabilizes training by constraining policy changes

## Intuition
- **Low KL Divergence:** Indicates similar distributions
- **High KL Divergence:** Indicates substantial differences, signaling potential issues in predictions or training

## Simple Example
- True distribution: $$P = [0, 0, 1, 0]$$
- Predicted distribution: $$Q = [0.1, 0.2, 0.6, 0.1]$$

$$
D_{KL}(P \parallel Q) = \log\frac{1}{0.6} \approx 0.5108
$$

This indicates room for improving the network's confidence in the correct class prediction.



