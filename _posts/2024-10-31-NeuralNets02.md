---
layout: post
title: "Notes on Neural Network"
date: 2024-11-01
excerpt: "Stochastic Gradient Descent, Embedding, and Softmax"
---

### Stochastic Gradient Descent (SGD) vs. Gradient Descent (GD)
- Advantages  
  - Computationally efficient for large datasets
  - Better exploration of the loss surface, helping avoid saddle points and poor local minima
  - Allows online learning when data arrives sequentially
- Disadvantages  
  - Noisy updates introduce variance, making convergence less stable
  - Requires careful hyperparameter tuning (e.g., learning rate scheduling)
  - Higher variance in updates leads to results varying across runs  

### Embedding Layer (and tie_word_embedding)
An embedding layer is a computationally efficient lookup table that maps token indices to dense vector representations. 

Given an input \( x \), it is tokenized into token indices \( z \) using a tokenizer. The embedding function then outputs a matrix of shape:

\[ \text{embedding}(z) \rightarrow (\text{max\_len}, d_{\text{model}}) \]

where:
- \( \text{max\_len} \) is the maximum sequence length (also called context length).
- \( d_{\text{model}} \) is the dimension of the model/embedding.

Thus, the embedding layer effectively acts as a lookup table of shape:

\[ (\text{vocab\_size}, d_{\text{model}}) \]

where \( \text{vocab\_size} \) is the tokenizer's vocabulary size.

**Tied Word Embeddings** refers to weight sharing between the embedding layer and the final projection layer (often called `lm_head`). Instead of having separate parameters for both, the embedding matrix \( (\text{vocab\_size}, d_{\text{model}}) \) is transposed and reused in the output layer \( (d_{\text{model}}, \text{vocab\_size}) \). This reduces the number of parameters and improves generalization.  

### Softmax and NLP
The softmax function converts raw output logits into a probability distribution over possible classes. It ensures that probabilities sum to 1, making it useful for classification.

\[
\sigma(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{N} e^{z_j}}
\]

where:
- \( z_i \) is the logit (raw score) from the model's output for class \( i \).
- \( e^{z_i} \) exponentiates the logit, ensuring positive values.
- The denominator normalizes the probabilities across \( N \) classes (e.g., vocabulary words in NLP).
- The output is a valid probability distribution where \( \sum \sigma(z_i) = 1 \).

### Cross-Entropy Loss
Cross-entropy loss measures the difference between the true and predicted probability distributions. It is defined as:

\[
L = -\sum_{i=1}^{N} y_i \log(\hat{y}_i)
\]

where:
- \( N \) is the number of classes.
- \( y_i \) is the true label (one-hot encoded, 1 for the correct class, 0 otherwise).
- \( \hat{y}_i \) is the predicted probability for class \( i \) (from softmax).
- The negative log penalizes incorrect predictions exponentially, encouraging higher confidence in correct predictions.

For a batch of size \( M \), the average cross-entropy loss is:

\[
L = -\frac{1}{M} \sum_{j=1}^{M} \sum_{i=1}^{N} y_{ji} \log(\hat{y}_{ji})
\]

where \( j \) indexes batch samples.