---
layout: post
title: "Neural Network Layers"
date: 2024-11-01
excerpt: "Attention, Dropout, Batch/Layer Normalization"
---


### Multi-Head Attention
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class ScaledDotProductAttention(nn.Module):
    def __init__(self, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(dropout)

    def forward(self, Q, K, V, mask=None):
        d_k = Q.size(-1)

        # scores : (batch_size, num_heads, seq_len, seq_len)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32)) 

        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))

        attn_weights = F.softmax(scores, dim=-1) 
        attn_weights = self.dropout(attn_weights)

        # (batch_size, num_heads, seq_len, d_k)
        output = torch.matmul(attn_weights, V) 
        return output, attn_weights

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__()
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        
        self.d_model = d_model # Embedding dimension
        self.num_heads = num_heads
        self.d_k = d_model // num_heads 
        
        self.W_q = nn.Linear(d_model, d_model, bias=False)
        self.W_k = nn.Linear(d_model, d_model, bias=False)
        self.W_v = nn.Linear(d_model, d_model, bias=False)

        self.fc_out = nn.Linear(d_model, d_model)
        self.attention = ScaledDotProductAttention(dropout)
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(d_model)
        
    def forward(self, Q, K, V, mask=None):
        batch_size = Q.shape[0]
        
        # src : (batch_size, seq_len, d_model)
        # after view : (batch_size, seq_len, num_head, d_model)
        # after transpose : (batch_size, num_head, seq_len, d_model)
        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        
        # mask : (batch_size, 1, seq_len, seq_len)
        attn_output, attn_weights = self.attention(Q, K, V, mask) # (batch_size, num_heads, seq_len, d_k)
        
        # Concatenate heads
        # attn_output : (batch_size, seq_len, d_model)
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        
        output = self.fc_out(attn_output)
        output = self.dropout(output)
        
        # Q : (batch_size, num_heads, seq_len, d_k)
        # After transpose and view : (batch_size, seq_len, d_model)
        return self.layer_norm(output + Q.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model))


class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.fc1 = nn.Linear(d_model, d_ff) # d_ff > d_model
        self.fc2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(d_model)

    def forward(self, x):
        residual = x
        x = self.fc1(x) # (batch_size, seq_len, d_ff)
        x = F.relu(x)
        x = self.fc2(x) # (batch_size, seq_len, d_model)
        x = self.dropout(x)
        return self.layer_norm(x + residual)


class TransformerEncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.attention = MultiHeadAttention(d_model, num_heads, dropout)
        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)

    def forward(self, x, mask=None):
        """
        x: Input tensor of shape (batch_size, seq_len, d_model)
        mask: Optional mask of shape (batch_size, 1, seq_len, seq_len)
        """
        x = self.attention(x, x, x, mask) # Self-attention
        x = self.feed_forward(x) # Position-wise feed-forward
        return x


# Example usage
if __name__ == "__main__":
    batch_size = 2
    seq_length = 5
    d_model = 512
    num_heads = 8
    d_ff = 2048

    x = torch.rand(batch_size, seq_length, d_model)
    encoder_layer = TransformerEncoderLayer(d_model, num_heads, d_ff)
    output = encoder_layer(x)
    print(output.shape)  # Expected: (batch_size, seq_length, d_model)
```
  
  
### Dropout
- A regularization technique that helps prevent overfitting.
- Randomly set the value of input zero, resulting in no forward pass or backward pass for gradient.
    - The remaining elements that are not set to zero are scaled by $$ \frac{1}{(1-p)} $$.
- During inference, dropoff it turned off and the matrix is used as-is.

```python
import torch

def dropout(x, p=0.5):
    mask = (torch.rand_like(x) > self.p).float()
    return mask * x / (1-p)
```
  
  
### Batch Normalization (batch_norm)
- Normalize the input features using their means and variances across a mini-batch.  
  
$$ \hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} $$  
      
$$ y = \gamma \hat{x} + \beta $$  
      
    - $$ \epsilon $$ is for numerical stability.
- Scale and shift the normalized values; The parameters are trainable.
- Reduces internal covairate shift, making the optimization landscape smoother.
    - When the distribution of activations changes during training process, it is difficult to converge.
    - batch norm normalizes the distribution of activations to mean zero and unit variance. 
- Normalized input distribution allows using higher learning rate values without causing divergence.
- During inference, it uses running estimates of mean and variance from the training iterations.
  
  
### Layer Normalization (layer_norm)
- Normalize feature across the feature per sample.
    - Doesn't require mini-batch.
    - Equation is the same with batch norm.
- Used primarily in NLP/Transformers as it is robust against varying sequence lengths.
- The original Transformer apply LayerNorm after attention/linear layer.
- Transformer block in GPT-2 apply LayerNorm before attenion/linear layer.
  
  
### Good old L1 and L2
- L1 regularization
    - Lasso
    - Sum of absolute values
    - Weights can be zero, meaning sparsity in feature or feature selection
- L2 regularization
    - Ridge
    - Penalty to large weight
    - Prevent overfitting by a few dominant features
- L1 or L2 terms are added to the standard loss function


