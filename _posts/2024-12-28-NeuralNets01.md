---
layout: post
title: "Random topics on Neural Network"
date: 2024-12-28
excerpt: "Stochastic Gradient Descent, Embedding, and Softmax"
---

### Stochastic Gradient Descent (SGD) vs. Gradient Descent (GD)

**Advantages**  

- Computationally efficient for large datasets  
- Better exploration of the loss surface, helping avoid saddle points and poor local minima  
- Allows online learning when data arrives sequentially  

**Disadvantages**  

- Noisy updates introduce variance, making convergence less stable  
- Requires careful hyperparameter tuning (e.g., learning rate scheduling)  
- Higher variance in updates leads to results varying across runs  

  

### Embedding Layer (and tie_word_embedding)
An embedding layer is a computationally efficient lookup table that maps token indices to dense vector representations. 

Given an input $$x$$, it is tokenized into token indices $$z$$ using a tokenizer. The embedding function then outputs a matrix of shape:

$$ \text{embedding}(z) \rightarrow (\text{max_len}, d_{\text{model}}) $$

where:
- $$\text{max_len}$$ is the maximum sequence length (also called context length).
- $$d_{\text{model}}$$ is the dimension of the model/embedding.

Thus, the embedding layer effectively acts as a lookup table of shape:

$$(\text{vocab_size}, d_{\text{model}})$$

where $$\text{vocab_size}$$ is the tokenizer's vocabulary size.  

**Tied Word Embeddings** refers to weight sharing between the embedding layer and the final projection layer (often called `lm_head`). Instead of having separate parameters for both, the embedding matrix $$ (\text{vocab_size}, d_{\text{model}}) $$ is transposed and reused in the output layer $$ (d_{\text{model}}, \text{vocab_size}) $$. This reduces the number of parameters and improves generalization.  
  

### Softmax and NLP
The softmax function converts raw output logits into a probability distribution over possible classes. It ensures that probabilities sum to 1, making it useful for classification.

$$
\sigma(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{N} e^{z_j}}
$$

where:
- $$z_i$$ is the logit (raw score) from the model's output for class $$i$$.
- $$e^{z_i}$$ exponentiates the logit, ensuring positive values.
- The denominator normalizes the probabilities across $$N$$  classes (e.g., vocabulary words in NLP).
- The output is a valid probability distribution where $$ \sum \sigma(z_i) = 1 $$.

### Cross-Entropy Loss
Cross-entropy loss measures the difference between the true and predicted probability distributions. It is defined as:

$$
L = -\sum_{i=1}^{N} y_i \log(\hat{y}_i)
$$

where:
- $$N$$ is the number of classes.
- $$y_i$$ is the true label (one-hot encoded, 1 for the correct class, 0 otherwise).
- $$ \hat{y}_i $$ is the predicted probability for class $$ i $$ (from softmax).
- The negative log penalizes incorrect predictions exponentially, encouraging higher confidence in correct predictions.

For a batch of size  $$M$$, the average cross-entropy loss is:

$$
L = -\frac{1}{M} \sum_{j=1}^{M} \sum_{i=1}^{N} y_{ji} \log(\hat{y}_{ji})
$$

where $$j$$ indexes batch samples.


### KL Divergence  

Kullbackâ€“Leibler (KL) divergence measures how one probability distribution diverges from another, defined as:

**Discrete case:**
$$
D_{KL}(P \parallel Q) = \sum_{x \in X} P(x)\log\frac{P(x)}{Q(x)}
$$

**Continuous case:**
$$
D_{KL}(P \parallel Q) = \int_{-\infty}^{\infty} P(x)\log\frac{P(x)}{Q(x)}\,dx
$$

### Properties
- **Asymmetric**: $$D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P)$$
- **Non-negative**: $$D_{KL}(P \parallel Q) \geq 0$$, equality if and only if $$P = Q$$

## Applications in Neural Networks

### 1. Variational Autoencoders (VAEs)
- Encourages approximate posterior $$Q(z|x)$$ to remain close to prior $$P(z)$$
- Loss:
$$
\text{Loss} = \text{Reconstruction Loss} + D_{KL}(Q(z|x)\parallel P(z))
$$

### 2. Knowledge Distillation
- Helps student network imitate teacher network's soft-label distributions
- Ensures close alignment between student and teacher predictions

### 3. Reinforcement Learning (Policy Gradients)
- Controls update magnitude of policies (e.g., TRPO, PPO)
- Stabilizes training by constraining policy changes

### Intuition
- **Low KL Divergence:** Indicates similar distributions
- **High KL Divergence:** Indicates substantial differences, signaling potential issues in predictions or training

### Simple Example
- True distribution: $$P = [0, 0, 1, 0]$$
- Predicted distribution: $$Q = [0.1, 0.2, 0.6, 0.1]$$

$$
D_{KL}(P \parallel Q) = \log\frac{1}{0.6} \approx 0.5108
$$

This indicates room for improving the network's confidence in the correct class prediction.


## Convex Optimization

Convex optimization involves minimizing a convex objective function subject to convex constraints. A function \(f(x)\) is convex if for all \(x, y\) and \(0 \leq \alpha \leq 1\):

$$
f(\alpha x + (1 - \alpha)y) \leq \alpha f(x) + (1 - \alpha)f(y)
$$

### Key Properties
- **Global Minimum:** Any local minimum is also a global minimum.
- **Efficient Algorithms:** Methods like gradient descent guarantee convergence to the global optimum.

### Standard Form
A typical convex optimization problem is defined as:

$$
\min_{x} f(x) \quad \text{subject to} \quad g_i(x) \leq 0, \quad h_j(x) = 0
$$

where \(f(x)\) and \(g_i(x)\) are convex, and \(h_j(x)\) are affine.

### Affine Function
An affine function is a linear function plus a constant, having the general form:

$$
h(x) = Ax + b
$$

where \(A\) is a matrix, \(x\) is a vector of variables, and \(b\) is a vector of constants.

### Role of Affine Functions in Convex Optimization
Affine functions play several crucial roles in convex optimization:

- **Constraints Simplification:** Define linear equality constraints, simplifying problems by restricting solutions to linear subspaces.
- **Preservation of Convexity:** Maintain the convexity of the feasible set, ensuring problems remain convex and efficiently solvable.
- **Algorithmic Efficiency:** Their linear structure allows efficient algorithmic implementations and guarantees reliable convergence properties.

### Applications in Neural Networks

### 1. Training (Gradient Descent)
- Minimizing loss functions (e.g., Mean Squared Error, Cross-Entropy) with gradient-based methods.
- Although neural network objectives are typically non-convex, local convex approximations allow efficient optimization via gradient descent.

### 2. Regularization
- Convex regularization (e.g., L1 and L2 norms) helps prevent overfitting.
- Promotes simpler, stable models with improved generalization.

### 3. Hyperparameter Optimization
- Convex optimization can guide efficient hyperparameter tuning (e.g., learning rates, regularization strengths).

### Example: Convex Loss Function
Mean Squared Error (MSE) is a convex loss function:

$$
\text{MSE}(y, \hat{y}) = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2
$$

Optimization via gradient descent ensures convergence to a global optimum when the model is linear or convex.

